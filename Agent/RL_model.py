# -*- coding: utf-8 -*-
"""生態系シミュレーション_QL付.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qcJ0aHHwcaOd3pg4mnZbhrQATriUkesA
"""

import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import os
import json
from RL_module import send_action_to_chatbot

class eco_simulator:
# =========================================
# 1. パラメータ設定
# =========================================
    def __init__(self):
        # --- 植物関連
        self.r1       = 0.35   # 植物の固有増殖率
        self.K1       = 500.0 # 植物の最大収容力
        self.alpha_12 = 0.02  # 植物が草食動物に食べられる率
        self.eps0     = 0.2   # 微生物による植物成長促進の係数

        # --- 草食動物関連
        self.beta_21  = 0.1    # 草食動物が植物を摂取した際の同化効率
        self.alpha_23 = 0.008  # 草食動物が肉食動物に捕食される率
        self.d2       = 0.05   # 草食動物の自然死亡率

        # --- 肉食動物関連
        self.beta_32  = 0.2   # 肉食動物が草食動物を摂取した際の同化効率
        self.d3       = 0.03  # 肉食動物の自然死亡率

        # --- 微生物関連
        self.beta_0   = 0.5   # 動物死骸からの微生物増殖効率
        self.d0       = 0.05  # 微生物の自然減衰率
        self.n0_star  = 50.0  # herbivores変動の基準となる微生物量

        # --- 土壌herbivores関連
        self.gamma_0  = -0.001  # 微生物が増えると酸性化すると想定 (負値)

        # --- 土壌herbivores関連 (修正ポイント)
        self.kappa    = 0.1     # pHが7に戻ろうとする強さ
        self.gamma_0  = -0.002  # 微生物が増えるほどpHを下げる

# =========================================
# 2. シミュレーション設定
# =========================================
    def initialize(self):

        self.T_max = 3000  # シミュレーションの総ステップ数（週）

        # --- 初期値を設定
        self.n0_0 = 10.0  # 微生物量
        self.n1_0 = 10.0  # 植物数
        self.n2_0 = 20.0  # 草食動物数
        self.n3_0 = 20.0   # 肉食動物数
        self.P_0  = 7.0   # pH初期値 (中性)

        # --- 記録用配列
        self.time_steps = np.arange(self.T_max+1)
        self.n0_vals = np.zeros(self.T_max+1)
        self.n1_vals = np.zeros(self.T_max+1)
        self.n2_vals = np.zeros(self.T_max+1)
        self.n3_vals = np.zeros(self.T_max+1)
        self.P_vals  = np.zeros(self.T_max+1)

        self.n0_vals2 = np.zeros(self.T_max+1)
        self.n1_vals2 = np.zeros(self.T_max+1)
        self.n2_vals2 = np.zeros(self.T_max+1)
        self.n3_vals2 = np.zeros(self.T_max+1)

        # --- 初期値を格納
        self.n0_vals[0] = self.n0_0
        self.n1_vals[0] = self.n1_0
        self.n2_vals[0] = self.n2_0
        self.n3_vals[0] = self.n3_0
        self.P_vals[0]  = self.P_0

    def reset_params(self):
        # --- 植物関連
        self.r1       = 0.35   # 植物の固有増殖率
        self.K1       = 500.0 # 植物の最大収容力
        self.alpha_12 = 0.02  # 植物が草食動物に食べられる率
        self.eps0     = 0.2   # 微生物による植物成長促進の係数

        # --- 草食動物関連
        self.beta_21  = 0.1    # 草食動物が植物を摂取した際の同化効率
        self.alpha_23 = 0.008  # 草食動物が肉食動物に捕食される率
        self.d2       = 0.05   # 草食動物の自然死亡率

        # --- 肉食動物関連
        self.beta_32  = 0.2   # 肉食動物が草食動物を摂取した際の同化効率
        self.d3       = 0.03  # 肉食動物の自然死亡率

        # --- 微生物関連
        self.beta_0   = 0.5   # 動物死骸からの微生物増殖効率
        self.d0       = 0.05  # 微生物の自然減衰率
        self.n0_star  = 50.0  # herbivores変動の基準となる微生物量

        # --- 土壌herbivores関連
        self.gamma_0  = -0.001  # 微生物が増えると酸性化すると想定 (負値)

        # --- 土壌herbivores関連 (修正ポイント)
        self.kappa    = 0.1     # pHが7に戻ろうとする強さ
        self.gamma_0  = -0.002  # 微生物が増えるほどpHを下げる

# =========================================
# 3. 更新関数
# =========================================
    def update(self, n0, n1, n2, n3, P, action=None):
        """
        1ステップ進める更新関数
        それぞれ次時刻の (n0, n1, n2, n3, P) を返す
        """
        if action is not None:
            if action == "肉食動物保護":
                    self.d3 -= 0.01
            elif action == "植物保全":
                self.r1 += 0.05
            elif action == "何もしない":
                pass
        # -------------------------
        # 微生物 n0(t+1)
        # -------------------------
        # 死骸由来の増分
        dead_from_predation = self.alpha_23 * n2 * n3  # 肉食による草食動物死
        dead_from_n2 = self.d2 * n2                    # 草食動物の自然死
        dead_from_n3 = self.d3 * n3                    # 肉食動物の自然死
        delta_n0 = self.beta_0 * (dead_from_predation + dead_from_n2 + dead_from_n3)
        # 減衰
        n0_next = n0 + delta_n0 - self.d0 * n0

        # -------------------------
        # 植物 n1(t+1)
        # -------------------------
        # 微生物による成長促進項 (飽和型)
        microbe_boost = self.eps0 * (n0 / (1.0 + n0))
        growth = self.r1 * (1.0 + microbe_boost) * n1 * (1.0 - n1 / self.K1)
        eaten  = self.alpha_12 * n1 * n2
        n1_next = n1 + growth - eaten

        # -------------------------
        # 草食動物 n2(t+1)
        # -------------------------
        # 植物摂食による増加
        increase_n2 = self.beta_21 * self.alpha_12 * n1 * n2
        # 肉食動物からの捕食
        # Holling II型捕食: pred_rate = alpha_23 * (n_2 / (1 + h * alpha_23 * n_2)) * n_3
        h = 0.02  # ハンドリングタイム
        pred_rate = self.alpha_23 * (n2 / (1.0 + h * self.alpha_23 * n2)) * n3
        # predation_n2 = alpha_23 * n2 * n3
        predation_n2 = pred_rate
        # 自然死亡
        natural_death_n2 = self.d2 * n2
        n2_next = n2 + increase_n2 - predation_n2 - natural_death_n2

        # -------------------------
        # 肉食動物 n3(t+1)
        # -------------------------
        # 草食動物捕食による増加
        # Holling II型捕食: pred_rate = alpha_23 * (n_2 / (1 + h * alpha_23 * n_2)) * n_3
        h = 0.02  # ハンドリングタイム
        pred_rate = self.alpha_23 * (n2 / (1.0 + h * self.alpha_23 * n2)) * n3
        # 捕食により草食動物が減る項は「pred_rate」として定義し，
        # 草食動物の更新式にも同じ pred_rate を使う。
        # ここでは predator=肉食, prey=草食 に相当

        # 肉食動物の増加は pred_rate * beta_32
        increase_n3 = self.beta_32 * pred_rate
        # 自然死亡
        natural_death_n3 = self.d3 * n3
        n3_next = n3 + increase_n3 - natural_death_n3

        # -------------------------
        # 土壌pH P(t+1)
        # -------------------------
        # (修正) 土壌がpH7に戻ろうとするバッファ + 微生物による酸性化
        P_next = P - self.kappa * (P - 7.0) + self.gamma_0 * n0

        self.reset_params()

        return n0_next, n1_next, n2_next, n3_next, P_next


# =========================================
# 5. メインループ
# =========================================
    def main(self):
        n0_current = self.n0_0
        n1_current = self.n1_0
        n2_current = self.n2_0
        n3_current = self.n3_0
        P_current  = self.P_0

        for t in range(self.T_max):
            n0_new, n1_new, n2_new, n3_new, P_new = self.update(
                n0_current, n1_current, n2_current, n3_current, P_current
            )
            # 記録
            self.n0_vals[t+1] = n0_new
            self.n1_vals[t+1] = n1_new
            self.n2_vals[t+1] = n2_new
            self.n3_vals[t+1] = n3_new
            self.P_vals[t+1]  = P_new

            # 更新
            n0_current = n0_new
            n1_current = n1_new
            n2_current = n2_new
            n3_current = n3_new
            P_current  = P_new

# =========================================
# 6. 結果の可視化
# =========================================

        # グラフを2つ並べる例： (上) n0, n1, n2, n3 の推移, (下) pH の推移
        fig, axs = plt.subplots(2, 1, figsize=(8, 8), sharex=True)

        # -- (1) 生物量の推移
        axs[0].plot(self.time_steps, self.n0_vals, label='Microbes (n0)')
        axs[0].plot(self.time_steps, self.n1_vals, label='Plants (n1)')
        axs[0].plot(self.time_steps, self.n2_vals, label='Herbivores (n2)')
        axs[0].plot(self.time_steps, self.n3_vals, label='Carnivores (n3)')

        axs[0].set_ylabel('Population / Biomass')
        axs[0].set_title('Population dynamics over time')
        axs[0].legend()
        axs[0].grid(True)

        # -- (2) pHの推移
        axs[1].plot(self.time_steps, self.P_vals, color='tab:green', label='Soil pH (P)')
        axs[1].axhline(7.0, color='gray', ls='--', alpha=0.5)  # pH7の目安線
        axs[1].set_ylabel('Soil pH')
        axs[1].set_xlabel('Time (weeks)')
        axs[1].set_title('Soil pH dynamics')
        axs[1].legend()
        axs[1].grid(True)

        plt.tight_layout()
        plt.show()

    def main_with_control(self, QL):
        """学習済みのQtableを使ってシミュレーションを行う"""
        n0_current = self.n0_0
        n1_current = self.n1_0
        n2_current = self.n2_0
        n3_current = self.n3_0
        P_current  = self.P_0

        QL.load_q_table()  # Qtable読み込み
        print("既存のQ-tableをロードしました。")

        for t in range(self.T_max):
            x_idx = QL.get_state_index(n1_current, n2_current, n3_current)
            # 行動選択
            action_idx, action = QL.get_action(x_idx)

            #選択した行動をユーザーに送信（API経由）
            send_action_to_chatbot(action)

            # 行動実行
            n0_new, n1_new, n2_new, n3_new, P_new = self.update(
                n0_current, n1_current, n2_current, n3_current, P_current, action
            )
            # 記録
            self.n0_vals[t+1] = n0_new
            self.n1_vals[t+1] = n1_new
            self.n2_vals[t+1] = n2_new
            self.n3_vals[t+1] = n3_new
            self.P_vals[t+1]  = P_new

            # 更新
            n0_current = n0_new
            n1_current = n1_new
            n2_current = n2_new
            n3_current = n3_new
            P_current  = P_new

        # 比較用に、Q-tableを使わない場合もシミュレーションを行う
        for t in range(self.T_max):
            n0_new, n1_new, n2_new, n3_new, P_new = self.update(
                n0_current, n1_current, n2_current, n3_current, P_current
            )
            # 記録
            self.n0_vals2[t+1] = n0_new
            self.n1_vals2[t+1] = n1_new
            self.n2_vals2[t+1] = n2_new
            self.n3_vals2[t+1] = n3_new

            # 更新
            n0_current = n0_new
            n1_current = n1_new
            n2_current = n2_new
            n3_current = n3_new
            P_current  = P_new


        # グラフを2つ並べる例： (上) n0, n1, n2, n3 の推移, (下) pH の推移
        fig, axs = plt.subplots(2, 1, figsize=(8, 8), sharex=True)

        # -- (1) 生物量の推移
        axs[0].plot(self.time_steps, self.n0_vals, label='Microbes (n0)')
        axs[0].plot(self.time_steps, self.n1_vals, label='Plants (n1)')
        axs[0].plot(self.time_steps, self.n2_vals, label='Herbivores (n2)')
        axs[0].plot(self.time_steps, self.n3_vals, label='Carnivores (n3)')
        axs[0].grid(True)

        axs[0].set_ylabel('Population / Biomass')
        axs[0].set_title('Population dynamics over time')
        # 凡例を追加
        axs[0].legend()

        # -- (2) Q-tableを使わない場合の生物量の推移
        axs[1].plot(self.time_steps, self.n0_vals2, label='Microbes (n0)')
        axs[1].plot(self.time_steps, self.n1_vals2, label='Plants (n1)')
        axs[1].plot(self.time_steps, self.n2_vals2, label='Herbivores (n2)')
        axs[1].plot(self.time_steps, self.n3_vals2, label='Carnivores (n3)')
        axs[1].grid(True)

        axs[1].set_ylabel('Population / Biomass')
        axs[1].set_title('Population dynamics over time (without Q-table)')

        axs[1].set_xlabel('Time (weeks)')
        plt.tight_layout()
        plt.show()

# =========================================
# 4. Q学習
# =========================================
class QLearner:
    def __init__(self):
        # デバッグ用jsonファイル名
        self.json_name = 'debug_data.json'

        # 学習パラメータ
        self.episode = 0
        self.num_episodes = 10000
        self.num_transition = 100
        self.epsilon = 0.05         # epsilon-greedy
        self.omega = 0.9           # 0.5 <= omega <= 1 で学習が収束する
        self.gamma = 0.95          # discount rate

        # Q学習周りの設定
        self.env = 1               # 基本は1でいい 0:env = Env(info) 1:env=pbcn.gym_ETC_PBCN(info)
        self.qtable = []           # Q-tableの初期値

        # 実験環境のパラメータ
        # self.diversity = 0.5
        # self.action_space = 2

        # アクションの定義
        self.action_list = ["肉食動物保護", "植物保全", "何もしない"]

        # グラフ描画用パラメータ
        self.graph_num_episode = 100
        self.graph_num_transition = 1000

        # 状態の閾値設定
        self.plants_thresholds = sorted([90, 80, 70, 60, 50, 40, 30])  # 例: 30%未満、30-60%、60%以上
        self.herbivores_thresholds = sorted([50, 40, 30, 20, 10])     # 例: 6.0未満、6.0-7.0、7.0以上
        self.calnivores_thresholds = sorted([15, 12, 9, 6, 3])     # 例: 1.0未満、1.0-2.0、2.0以上

        # 状態空間の定義
        self.state_dimensions = 3  # 植物、草食動物、肉食動物の3次元
        self.state_space = (len(self.plants_thresholds) + 1) * (len(self.herbivores_thresholds) + 1) * (len(self.calnivores_thresholds) + 1)

        # Q学習用の変数
        self.Q_table = None
        self.transition_count = None
        self.episode_rewards = []

    def initialize_q_table(self, lower_bound=0.0, upper_bound=0.1):
        """Q-tableの初期化"""
        action_space = len(self.action_list)
        self.Q_table = np.random.uniform(lower_bound, upper_bound, (self.state_space, action_space))
        self.transition_count = np.zeros(self.state_space, dtype=int)
        self.episode_rewards = []

    def classify_value(self, value, thresholds):
        """値をしきい値に基づいて分類"""
        for i, threshold in enumerate(thresholds):
            if value < threshold:
                return i
        return len(thresholds)

    def get_state_index(self, plants, herbivores, calnivores):
        """センサー値から状態インデックスを計算"""
        # 各パラメータを3状態に分類
        herbivores_state = self.classify_value(plants, self.plants_thresholds)
        herbivores_state = self.classify_value(herbivores, self.herbivores_thresholds)
        calnivores_state = self.classify_value(calnivores, self.calnivores_thresholds)

        # 3進数的な考え方で状態インデックスを計算
        # 例: herbivores=2, ph=1, ec=0 の場合、
        # state_index = 2*(3^2) + 1*(3^1) + 0*(3^0) = 18 + 3 + 0 = 21
        state_index = (herbivores_state * (self.state_dimensions ** 2) +
                    herbivores_state * self.state_dimensions +
                    calnivores_state)

        return state_index

    def get_state_description(self, state_index):
        """状態インデックスから各パラメータの状態を取得（デバッグ用）"""
        herbivores_state = state_index // (self.state_dimensions ** 2)
        herbivores_state = (state_index % (self.state_dimensions ** 2)) // self.state_dimensions
        calnivores_state = state_index % self.state_dimensions

        herbivores_desc = ['低', '中', '高'][herbivores_state]
        herbivores_desc = ['酸性', '中性', 'アルカリ性'][herbivores_state]
        calnivores_desc = ['低', '中', '高'][calnivores_state]

        return f"湿度: {herbivores_desc}, pH: {herbivores_desc}, EC: {calnivores_desc}"

    def get_action(self, state_idx):
        """ε-グリーディ方策に基づいて行動を選択"""
        if np.random.rand() < self.epsilon:  # 確率εでランダムな行動
            action_idx = np.random.randint(0, len(self.action_list))
        else:
            action_idx = np.argmax(self.Q_table[state_idx])

        action = self.action_list[action_idx]

        return action_idx, action

    def calculate_reward(self, calnivores):
        """報酬を計算"""
        value = calnivores
        return value

    def update_q_value(self, pre_state, action, current_state, reward, alpha):
        """Q値の更新"""
        if pre_state is not None:
            self.Q_table[pre_state, action] = (
                self.Q_table[pre_state, action] +
                alpha * (reward + self.gamma * np.max(self.Q_table[current_state]) -
                        self.Q_table[pre_state, action])
            )

    def learn(self, eco_sim, target_herbivores=45, target_ph=6.5, target_ec=1.5):
        """Q学習の実行"""
        self.initialize_q_table()
        Tau = np.zeros(self.state_space)  # 各状態の訪問回数

        for episode in range(self.num_episodes):
            # ランダムな初期状態の生成
            n0_current = eco_sim.n0_0 # 微生物量
            n1_current = eco_sim.n1_0 # 植物数 plants
            n2_current = eco_sim.n2_0  # 草食動物数 herbivores
            n3_current = eco_sim.n3_0 # 肉食動物数 calnivores
            P_current  = eco_sim.P_0  # pH初期値 (中性)

            pre_x_idx = None
            x_idx = self.get_state_index(n1_current, n2_current, n3_current)
            alpha = 1/((episode+1)**self.omega)

            for transition in range(self.num_transition):
                # 行動の選択
                action_idx, action = self.get_action(x_idx)

                # 行動に基づく環境の変化をシミュレート（実際の環境に合わせて調整が必要）
                if action == "肉食動物保護":
                    eco_sim.d3 -= 0.01
                elif action == "植物保全":
                    eco_sim.r1 += 0.05
                elif action == "何もしない":
                    pass

                # 値の範囲を制限
                # herbivores = np.clip(herbivores, 0, 100)
                # ph = np.clip(ph, 4.0, 9.0)
                # ec = np.clip(ec, 0.0, 3.0)

                # 新しい状態の取得
                n0_new, n1_new, n2_new, n3_new, P_new = eco_sim.update(
        n0_current, n1_current, n2_current, n3_current, P_current
    )

                # 報酬の計算
                reward = self.calculate_reward(n3_new)

                # Q値の更新
                self.update_q_value(pre_x_idx, action_idx, x_idx, reward, alpha)

                pre_x_idx = x_idx

                # 1000エピソードごとに表示
                if episode % self.graph_num_episode == 0 and transition % self.graph_num_transition == 0:
                    print(f'\r episode:{episode}/{self.num_episodes} '
                        f'step: {transition+1}, '
                        f'報酬: {reward:.2f}, '
                        f'状態: {self.get_state_description(x_idx)}, '
                        f'行動: {action}  ', end='', flush=True)

                # 状態の更新
                x_idx = self.get_state_index(n1_new, n2_new, n3_new)
                # シミュレーションの状態を更新
                n0_current = n0_new
                n1_current = n1_new
                n2_current = n2_new
                n3_current = n3_new
                P_current = P_new

            self.episode_rewards.append(reward)

        return self.Q_table, Tau, self.transition_count, self.episode_rewards

    def save_q_table(self, filename='q_table.json'):
        """Q-tableをJSONファイルに保存"""
        with open(filename, 'w') as f:
            json.dump({
                'Q_table': self.Q_table.tolist(),
                'parameters': {
                    'episode': self.episode,
                    'num_episodes': self.num_episodes,
                    'num_transition': self.num_transition,
                    'epsilon': self.epsilon,
                    'omega': self.omega,
                    'gamma': self.gamma,
                    'plants_thresholds': self.plants_thresholds,
                    'herbivores_thresholds': self.herbivores_thresholds,
                    'calnivores_thresholds': self.calnivores_thresholds
                }
            }, f)

    def load_q_table(self, filename='q_table.json'):
        """JSONファイルからQ-tableを読み込み"""
        with open(filename, 'r') as f:
            data = json.load(f)
            self.Q_table = np.array(data['Q_table'])
            self.episode = data['parameters']['episode']
            params = data['parameters']
            self.num_episodes = params['num_episodes']
            self.num_transition = params['num_transition']
            self.epsilon = params['epsilon']
            self.omega = params['omega']
            self.gamma = params['gamma']
            self.plants_thresholds = params['plants_thresholds']
            self.herbivores_thresholds = params['herbivores_thresholds']
            self.calnivores_thresholds = params['calnivores_thresholds']

if __name__ == '__main__':
    eco_sim = eco_simulator()
    eco_sim.initialize()
    QL = QLearner()

    QL.learn(eco_sim)
    QL.save_q_table()
    eco_sim.main_with_control(QL)

